{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUfSxuMTSuG+atU3HeoOWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/CNN-Architectures/blob/master/Implementations/AlexNet/AlexNet_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBXyG54I-KPL",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of AlexNet\n",
        "\n",
        "We will use the [tensorflow.keras Functional API](https://www.tensorflow.org/guide/keras/functional) to build AlexNet from the original paper: “[ImageNet Classification with Deep Convolutional\n",
        "Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)” by Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton.\n",
        "\n",
        "[Video tutorial](https://www.youtube.com/watch?v=rFpzCPcI6O0&list=PLaPdEEY26UXyE3UchW0C742xh542yh0yI&index=1)\n",
        "\n",
        "---\n",
        "\n",
        "In the paper we can read:\n",
        "\n",
        ">**[i]** “The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.”\n",
        ">\n",
        ">**[ii]** “We applied this normalization after applying the ReLU nonlinearity in certain layers.”\n",
        ">\n",
        ">**[iii]** “If we set s < z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3.”\n",
        ">\n",
        ">**[iv]** “The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map).\"\n",
        ">\n",
        ">**[v]** \"The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48.\n",
        ">\n",
        ">**[vi]** \"The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer.”\n",
        ">\n",
        ">**[vii]** ”The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.”\n",
        ">\n",
        ">**[viii]** \"We use dropout in the first two fully-connected layers [...]\"\n",
        "\n",
        "<br>\n",
        "\n",
        "We will also use the following Diagram **[ix]**:\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/Machine-Learning-Tokyo/DL-workshop-series/master/Part%20I%20-%20Convolution%20Operations/images/AlexNet.png width=\"800\">\n",
        "\n",
        "---\n",
        "\n",
        "## Network architecture\n",
        "\n",
        "- The network consists of 5 *Convolutional* layers and 3 *Fully Connected* Layers (**[ix]**)\n",
        "\n",
        "- *Max Pooling* is applied Between the layers:\n",
        " - 1conv-2conv (**[v]**)\n",
        " - 2conv-3conv (**[vi]**)\n",
        " - 5conv-1fc (**[ix]**)\n",
        "\n",
        "- Before Max Pooling a normalization technique is applied. At the paper a normalization method named LRN (Local Response Normalization) was used. However, since LRN is not part of the standard *tensorflow.keras* library and it is not in the scope of this section to teach how to write custom layers, we will use another method instead. We chose to replace LRN with *Batch Normalization* for this example.\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow\n",
        "We will:\n",
        "1. import the neccesary layers\n",
        "2. demonstrate how tensors and tf.keras layers work \n",
        "3. write the code for the first block\n",
        "4. write the code for the blocks 2-5 (*Convolution blocks*)\n",
        "5. write the code for the hidden *Fully Connected blocks*\n",
        "6. write the code for the output *Fully Connected blocks*\n",
        "7. build the model\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd1JYxKX-fJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, \\\n",
        "     BatchNormalization, MaxPool2D, Flatten, Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4NV3s7G-j0j",
        "colab_type": "text"
      },
      "source": [
        "### 2. *Tensors* and *Layers*\n",
        "The idea is to build a graph of tensors which are connected through layers. We will start with the input tensor which is created by an `Input()` layer.\n",
        "\n",
        "In the `Input()` layer we have to define the *shape* of the input object (e.g. a numpy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJN10mY-qhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(224, 224, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysP5ioE--s0g",
        "colab_type": "text"
      },
      "source": [
        "`input` is a tensorflow Tensor object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8fGzLGY-xSt",
        "colab_type": "code",
        "outputId": "d5d5772e-8024-4b9b-9479-2d3bbcddffd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "type(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DPtL9Oy-8Wr",
        "colab_type": "text"
      },
      "source": [
        "### 3. 1st block\n",
        "Now we can pass this tensor to another *layer* object and the output will be another tensor.\n",
        "\n",
        "The first layer that we will apply is a *Convolutional* layer (`Conv2D()`)\n",
        "\n",
        "In this layer we will pass:\n",
        "- the number of filters\n",
        "- the kernel size\n",
        "- the strides\n",
        "- the padding\n",
        "- the activation function\n",
        "\n",
        "From the paper:\n",
        "\n",
        ">The first convolutional layer filters the 224×224×3 input image with **96 kernels** of size **11×11**×3 with a **stride of 4** pixels (**[iv]**)\n",
        "\n",
        ">The **ReLU** non-linearity is applied to the output of every convolutional and fully-connected layer (**[i]**)\n",
        "\n",
        "*Note: at each reference we mark with **bold** the parts that are informative for the coding of the corresponding block.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAYDOpl_B38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=96,\n",
        "          kernel_size=11,\n",
        "          strides=4,\n",
        "          padding='same',\n",
        "          activation='relu')(input)  # 1st convolutional layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFT1-Vqo_E4W",
        "colab_type": "text"
      },
      "source": [
        "Before the second Convolutional layer we have to normalize and pool the output of the first Convolutional layer:\n",
        "\n",
        "> \"The second convolutional layer takes as input the (**response-normalized and pooled**) output of the first convolutional layer\" (**[v]**)\n",
        "\n",
        "Regarding the `MaxPool2D()` layer we have to pass:\n",
        "\n",
        "- the pool size\n",
        "- the strides\n",
        "\n",
        "\n",
        "From the paper:\n",
        "\n",
        ">If we set s < z, we obtain overlapping pooling. This is what we use throughout our network, with **s = 2 and z = 3** (**[iii]**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We9HLInT_Mdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LbZxkI_RaN",
        "colab_type": "text"
      },
      "source": [
        "Notice that it is ok to use the same name `x` for the output tensor of different layers as long as we implement with the correct order.\n",
        "\n",
        "<br>\n",
        "\n",
        "So the first block is now:\n",
        "\n",
        "```python\n",
        "x = Conv2D(filters=96,\n",
        "          kernel_size=11,\n",
        "          strides=4,\n",
        "          padding='same',\n",
        "          activation='relu')(input)  # 1st convolutional layer\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIYX3i4Z_V61",
        "colab_type": "text"
      },
      "source": [
        "### 4. 2nd-5th block\n",
        "\n",
        "From the paper:\n",
        "\n",
        ">The **second** convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with **256 kernels of size 5 × 5** × 48 (**[v]**)\n",
        "\n",
        "*Note: The **48** as the size of the last dimension of the kernel is technically 96 in our case. This is because the input tensor has 96 feature maps, equal to the number of kernels of the previous layer. However, since the original network was implemented in two GPUs in parallel, each one had 48 feature maps at this stage.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpdrlVEn_VWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=256,\n",
        "          kernel_size=5,\n",
        "          padding='same',\n",
        "          activation='relu')(x)  # 2nd convolutional layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLkfnk-e_cPo",
        "colab_type": "text"
      },
      "source": [
        "From the paper:\n",
        "\n",
        ">The **third** convolutional layer has **384 kernels of size 3 × 3** × 256 connected to the (**normalized, pooled**) outputs of the second convolutional layer. (**[vi]**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgGCGq__c3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "x = Conv2D(filters=384,\n",
        "          kernel_size=3,\n",
        "          padding='same',\n",
        "          activation='relu')(x)  # 3rd convolutional layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QXcQo1d_kg1",
        "colab_type": "text"
      },
      "source": [
        "From the paper:\n",
        "\n",
        ">The **fourth** convolutional layer has **384 kernels of size 3 × 3** × 192 (**[vii]**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHR4Lds_njb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=384,\n",
        "          kernel_size=3,\n",
        "          padding='same',\n",
        "          activation='relu')(x)  # 4th convolutional layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IULi3Ec_oM7",
        "colab_type": "text"
      },
      "source": [
        "From the paper:\n",
        "\n",
        ">the **fifth** convolutional layer has **256 kernels of size 3 × 3** × 192. (**[vii]**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwFe9dI8_rh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=256,\n",
        "          kernel_size=3,\n",
        "          padding='same',\n",
        "          activation='relu')(x)  # 5th convolutional layer\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaMHzA-x_tsP",
        "colab_type": "text"
      },
      "source": [
        "### 5. Dense layers\n",
        "\n",
        "Before passing the output tensor of the last Convolutional layer (13x13x256) to the first `Dense()` layer we have to flatten it to a one-dimension tensor. We do it by using the `Flatten()` layer.\n",
        "\n",
        "*Note: you may find the (one-dimension) tensors to also be called vectors. We use the word tensor as reference to the tensorflow.Tensor object of which it is an instance.*\n",
        "\n",
        "From the paper:\n",
        "\n",
        ">The **fully-connected** layers have **4096** neurons each (**[vii]**)\n",
        ">Dropout with drop probability 0.5 is used after the first 2 Fully Connected layers (**[viii]**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS89xZOC_xry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Flatten()(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "x = Dropout(rate=0.5)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7tEuxs_1gv",
        "colab_type": "text"
      },
      "source": [
        "### 6. Output layer\n",
        "\n",
        "Since the model is to be used for classifiction tasks, the output of the model will be a `Dense()` layer with:\n",
        "- number of units equal to the number of classes in our task which are 1000 based on **[ix]**\n",
        "- `softmax` actication if we target to only one class per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my0-CROq_4Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = Dense(units=1000, activation='softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7hzw3yI_6oa",
        "colab_type": "text"
      },
      "source": [
        "### 7. Model\n",
        "\n",
        "In order to build the *model* we will use the `tensorflow.keras.Model` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-mbDfd_8ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8LBuT7Z_-Wf",
        "colab_type": "text"
      },
      "source": [
        "To define the model we need the input tensor(s) and the output tensor(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rx83M3AABz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=input, outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spWYEcwiADKG",
        "colab_type": "text"
      },
      "source": [
        "## Final code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhGnPrnAKIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, \\\n",
        "     BatchNormalization, MaxPool2D, Flatten, Dense, Dropout\n",
        "\n",
        "input = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = Conv2D(filters=96,\n",
        "           kernel_size=11,\n",
        "           strides=4,\n",
        "           padding='same',\n",
        "           activation='relu')(input)  # 1st convolutional layer\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "x = Conv2D(filters=256,\n",
        "           kernel_size=5,\n",
        "           padding='same',\n",
        "           activation='relu')(x)  # 2nd convolutional layer\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "x = Conv2D(filters=384,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           activation='relu')(x)  # 3rd convolutional layer\n",
        "\n",
        "x = Conv2D(filters=384,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           activation='relu')(x)  # 4th convolutional layer\n",
        "\n",
        "x = Conv2D(filters=256,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           activation='relu')(x)  # 5th convolutional layer\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "x = Dropout(rate=0.5)(x)\n",
        "\n",
        "output = Dense(units=1000, activation='softmax')(x)\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "model = Model(inputs=input, outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxv2Y6B4AMBp",
        "colab_type": "text"
      },
      "source": [
        "## Model diagram\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Machine-Learning-Tokyo/CNN-Architectures/master/Implementations/AlexNet/AlexNet_diagram.svg?sanitize=true\">"
      ]
    }
  ]
}